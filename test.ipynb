{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ImageBind - Inference Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import *\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from imagebind import data\n",
    "from imagebind.models import imagebind_model\n",
    "from imagebind.models.imagebind_model import ModalityType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = [\"A dog.\", \"A car\", \"A bird\"]\n",
    "image_paths = [\n",
    "    \".assets/dog_image.jpg\",\n",
    "    \".assets/car_image.jpg\",\n",
    "    \".assets/bird_image.jpg\",\n",
    "]\n",
    "audio_paths = [\n",
    "    \".assets/dog_audio.wav\",\n",
    "    \".assets/car_audio.wav\",\n",
    "    \".assets/bird_audio.wav\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Instantiate model\n",
    "model = imagebind_model.imagebind_huge(pretrained=True)\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['text', 'vision', 'audio'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "inputs = {\n",
    "    ModalityType.TEXT: data.load_and_transform_text(text_list, device),\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(image_paths, device),\n",
    "    ModalityType.AUDIO: data.load_and_transform_audio_data(audio_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    embeddings: Dict[str, torch.Tensor] = model(inputs)\n",
    "\n",
    "embeddings.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_text, car_text, bird_text = embeddings[ModalityType.TEXT]\n",
    "dog_image, car_image, bird_image = embeddings[ModalityType.VISION]\n",
    "dog_audio, car_audio, bird_audio = embeddings[ModalityType.AUDIO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.item()=0.23663438856601715\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = F.cosine_similarity(dog_text, dog_image, dim=-1)\n",
    "print(f\"{cosine_sim.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.item()=0.12194865942001343\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = F.cosine_similarity(dog_text, dog_audio, dim=-1)\n",
    "print(f\"{cosine_sim.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.item()=0.1972985714673996\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = F.cosine_similarity(dog_image, dog_audio, dim=-1)\n",
    "print(f\"{cosine_sim.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.item()=0.044254571199417114\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = F.cosine_similarity(dog_text, bird_audio, dim=-1)\n",
    "print(f\"{cosine_sim.item()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Disparity vs Depth Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image_paths: List[str] = [\n",
    "    \"/path/to/000_disparity.png\",\n",
    "    \"/path/to/000_depth.png\",\n",
    "    \"/path/to/000.png\",\n",
    "]\n",
    "\n",
    "# Load data\n",
    "my_inputs = {\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(my_image_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    my_embeddings: Dict[str, torch.Tensor] = model(my_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity, depth, img = my_embeddings[ModalityType.VISION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.item()=0.6755827069282532\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = F.cosine_similarity(img, disparity, dim=-1)\n",
    "print(f\"{cosine_sim.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.item()=0.6359966397285461\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = F.cosine_similarity(img, depth, dim=-1)\n",
    "print(f\"{cosine_sim.item()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Disparity maps show greater similarity with input image than depth maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E.g. 2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_image_paths: List[str] = [\n",
    "    \"/path/to/008_disparity.png\",\n",
    "    \"/path/to/008_depth.png\",\n",
    "    \"/path/to/008.png\",\n",
    "]\n",
    "\n",
    "# Load data\n",
    "my_inputs = {\n",
    "    ModalityType.VISION: data.load_and_transform_vision_data(my_image_paths, device),\n",
    "}\n",
    "\n",
    "with torch.no_grad():\n",
    "    my_embeddings: Dict[str, torch.Tensor] = model(my_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "disparity, depth, img = my_embeddings[ModalityType.VISION]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.item()=0.8093360066413879\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = F.cosine_similarity(img, disparity, dim=-1)\n",
    "print(f\"{cosine_sim.item()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine_sim.item()=0.7142566442489624\n"
     ]
    }
   ],
   "source": [
    "cosine_sim = F.cosine_similarity(img, depth, dim=-1)\n",
    "print(f\"{cosine_sim.item()=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Disparity maps show much greater similarity with input image than depth maps"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imagebind",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
